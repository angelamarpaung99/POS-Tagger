{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0tqHrvIwsxL"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import io\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_init_table(fname):\n",
    "    tag_count = {}\n",
    "    tag_count['<start>'] = 0\n",
    "    word_tag = {}\n",
    "    tag_trans = {}\n",
    "    idx_line = 0\n",
    "    is_first_word = 0\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content.insert(0, (\"<kalimat id=1>\"))\n",
    "    i = 2\n",
    "    for j in range(len(content)):\n",
    "        if content[j] == '':\n",
    "            content[j] = \"</kalimat>\"\n",
    "            content.insert(j+1, \"<kalimat id=\"+str(i)+\">\")\n",
    "            i+= 1\n",
    "    \n",
    "    while idx_line < len(content):\n",
    "        if not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                if content[idx_line] != '':\n",
    "                    content_part = content[idx_line].split('\\t')\n",
    "                    if content_part[1] in tag_count:\n",
    "                        tag_count[content_part[1]] += 1\n",
    "                    else:\n",
    "                        tag_count[content_part[1]] = 1\n",
    "\n",
    "                    current_word_tag = content_part[0]+'|'+content_part[1]\n",
    "                    if current_word_tag in word_tag:\n",
    "                        word_tag[current_word_tag] += 1\n",
    "                    else:    \n",
    "                        word_tag[current_word_tag] = 1\n",
    "\n",
    "                    if is_first_word == 1:\n",
    "                        current_tag_trans = '<start>,'+content_part[1]\n",
    "                        is_first_word = 0\n",
    "                    else:\n",
    "                        current_tag_trans = prev_tag+','+content_part[1]\n",
    "\n",
    "                    if current_tag_trans in tag_trans:\n",
    "                        tag_trans[current_tag_trans] += 1\n",
    "                    else:\n",
    "                        tag_trans[current_tag_trans] = 1                    \n",
    "                    prev_tag = content_part[1]   \n",
    "                \n",
    "            else:\n",
    "                tag_count['<start>'] += 1\n",
    "                is_first_word = 1\n",
    "            idx_line = idx_line + 1\n",
    "\n",
    "        idx_line = idx_line+1       \n",
    "    return tag_count, word_tag, tag_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4Xa_aXlwsxX",
    "outputId": "695523cb-40e2-43fd-a798-15faa1bf3464",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_count, word_tag, tag_trans = read_file_init_table('Indonesian_Manually_Tagged_Corpus.txt')\n",
    "print(\"Tag count : \")\n",
    "print(tag_count)\n",
    "print(\"\\nWord and it's tag : \")\n",
    "print(word_tag)\n",
    "print(\"\\nTag transition : \")\n",
    "print(tag_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9i5zqkWwsxc"
   },
   "outputs": [],
   "source": [
    "def create_trans_prob_table(tag_trans, tag_count):\n",
    "    trans_prob = {}\n",
    "    for tag1 in tag_count.keys():\n",
    "        for tag2 in tag_count.keys():\n",
    "            trans_idx = tag1+','+tag2\n",
    "            if trans_idx in tag_trans:\n",
    "                trans_prob[trans_idx] = tag_trans[trans_idx]/tag_count[tag1]\n",
    "    return trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqIZemLYwsxg"
   },
   "outputs": [],
   "source": [
    "trans_prob = create_trans_prob_table(tag_trans, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-WER7wSwsxj"
   },
   "outputs": [],
   "source": [
    "def create_emission_prob_table(word_tag, tag_count):\n",
    "    emission_prob = {}\n",
    "    for word_tag_entry in word_tag.keys():\n",
    "        word_tag_split = word_tag_entry.split('|')\n",
    "        if word_tag_split[0] != '' and word_tag_split[1] != '':\n",
    "            current_word = word_tag_split[0]\n",
    "            current_tag = word_tag_split[1]\n",
    "            emission_key = current_word+','+current_tag\n",
    "            emission_prob[emission_key] = word_tag[word_tag_entry]/tag_count[current_tag]    \n",
    "    return emission_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmv8OFUlwsxm"
   },
   "outputs": [],
   "source": [
    "emission_prob = create_emission_prob_table(word_tag, tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE METHOD\n",
    "def baselineMethod(sentence, word_tag):\n",
    "    tag_sequence = []\n",
    "    sentence_words = sentence.split()\n",
    "    word_key = [key.split('|') for key in word_tag.keys()]\n",
    "    # word_tag[]\n",
    "    for i in sentence_words:\n",
    "        val = 0\n",
    "        curr_tag = ''\n",
    "        for w in word_key:\n",
    "            if i == w[0]:\n",
    "                if val < word_tag[w[0]+'|'+w[1]]:\n",
    "                    curr_tag = w[1]\n",
    "                    val = word_tag[w[0]+'|'+w[1]]\n",
    "        tag_sequence.append(curr_tag)\n",
    "    return tag_sequence\n",
    "\n",
    "tag_count, word_tag, tag_trans = read_file_init_table('Indonesian_Manually_Tagged_Corpus.txt')\n",
    "tag_count_1, word_tag_1, tag_trans_1 = read_file_init_table('sample_postagged.txt')\n",
    "kalimat = [\"Kera untuk amankan pesta olahraga .\",\"Pemerintah kota Delhi mengerahkan monyet untuk mengusir monyet-monyet lain yang berbadan lebih kecil dari arena Pesta Olahraga Persemakmuran .\", \"Beberapa laporan menyebutkan setidaknya 10 monyet ditempatkan di luar arena lomba dan pertandingan di ibukota India .\", \"Pemkot Delhi memiliki 28 monyet dan berencana mendatangkan 10 monyet sejenis dari negara bagian Rajasthan .\", \"Jumlah monyet di ibukota India mencapai ribuan , sebagian besar , berada di kantor-kantor pemerintah dan hewan ini dianggap mengganggu ketertiban umum .\", \"Jenis monyet yang dikerahkan pemkot berbadan besar , berkekor panjang , dan memiliki wajah berwarna hitam .\", \"Monyet ini diikat dengan tali panjang dan pelatih yang mengawasi mereka akan melepas tali begitu monyet-monyet lain mendekat .\", \"Kantor berita AFP melaporkan stadion tinju dan hockey mendapatkan perhatian khusus karena sering diserbu monyet .\", \"Monyet besar akan dikerahkan di dua stadion untuk mengusir serbuan monyet kecil .\", \"Monyet besar akan dikerahkan di dua stadion tersebut untuk mengusir serbuan monyet kecil , kata Devender Prasad , pejabat pemerintah kota Delhi kepada kantor berita AFP .\"]\n",
    "\n",
    "accuracy = []\n",
    "for i in range(len(kalimat)):\n",
    "    print(\"\\nKalimat \",i+1, \" : \", kalimat[i])\n",
    "    prediction = baselineMethod(kalimat[i], word_tag)\n",
    "    print(\"\\nPOS tag menggunakan baseline method : \\n\",prediction)\n",
    "    \n",
    "    real = baselineMethod(kalimat[i], word_tag_1)\n",
    "    print(\"POS tag berdasarkan sampel: \\n\",real)\n",
    "\n",
    "    true = 0\n",
    "    for j in range(len(prediction)):\n",
    "        if(prediction[j] == real[j]):\n",
    "            true+=1\n",
    "    accuracy.append(true/len(prediction)*100)\n",
    "    print(\"Akurasi = %.2f\" % accuracy[i])\n",
    "\n",
    "print(\"\\nRata-rata akurasi = %.2f\" % (sum(accuracy)/len(kalimat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICAL METHOD\n",
    "def read_dataset(fname):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content.insert(0, (\"<kalimat id=1>\"))\n",
    "    i = 2\n",
    "    for j in range(len(content)):\n",
    "        if content[j] == '':\n",
    "            content[j] = \"</kalimat>\"\n",
    "            content.insert(j+1, \"<kalimat id=\"+str(i)+\">\")\n",
    "            i+= 1\n",
    "    idx_line = 0\n",
    "    while idx_line < len(content):\n",
    "        sent = []\n",
    "        tag = []\n",
    "        if not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                if content[idx_line] != '':\n",
    "                    content_part = content[idx_line].split('\\t')\n",
    "                    sent.append(content_part[0])\n",
    "                    tag.append(content_part[1])\n",
    "                \n",
    "            idx_line = idx_line + 1\n",
    "        sentences.append(sent)\n",
    "        tags.append(tag)\n",
    "        idx_line = idx_line+2        \n",
    "    return sentences, tags\n",
    "\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    }\n",
    "\n",
    "\n",
    " \n",
    "def transform_to_dataset(sentences, tags):\n",
    "    X, y = [], []\n",
    " \n",
    "    for sentence_idx in range(len(sentences)):\n",
    "        for index in range(len(sentences[sentence_idx])):\n",
    "            X.append(features(sentences[sentence_idx], index))\n",
    "            y.append(tags[sentence_idx][index])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "\n",
    "sentences,tags = read_dataset('Indonesian_Manually_Tagged_Corpus.txt')\n",
    "\n",
    "# Split the dataset for training and testing\n",
    "cutoff = int(.75 * len(sentences))\n",
    "training_sentences = sentences[:cutoff]\n",
    "test_sentences = sentences[cutoff:]\n",
    "training_tags = tags[:cutoff]\n",
    "test_tags = tags[cutoff:]\n",
    "     \n",
    "X, y = transform_to_dataset(training_sentences, training_tags)\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', tree.DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "clf.fit(X, y)   \n",
    " \n",
    "print('Training completed')\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences, test_tags)\n",
    " \n",
    "print(\"Accuracy:\")\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# Test model yang sudah dilatih dengan kalimat masukan bebas\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    #return zip(sentence, tags)\n",
    "    return tags\n",
    "\n",
    "# kalimat = [\"Kera untuk amankan pesta olahraga .\",\"Pemerintah kota Delhi mengerahkan monyet untuk mengusir monyet-monyet lain yang berbadan lebih kecil dari arena Pesta Olahraga Persemakmuran .\", \"Beberapa laporan menyebutkan setidaknya 10 monyet ditempatkan di luar arena lomba dan pertandingan di ibukota India .\", \"Pemkot Delhi memiliki 28 monyet dan berencana mendatangkan 10 monyet sejenis dari negara bagian Rajasthan .\", \"Jumlah monyet di ibukota India mencapai ribuan , sebagian besar , berada di kantor-kantor pemerintah dan hewan ini dianggap mengganggu ketertiban umum .\", \"Jenis monyet yang dikerahkan pemkot berbadan besar , berkekor panjang , dan memiliki wajah berwarna hitam .\", \"Monyet ini diikat dengan tali panjang dan pelatih yang mengawasi mereka akan melepas tali begitu monyet-monyet lain mendekat .\", \"Kantor berita AFP melaporkan stadion tinju dan hockey mendapatkan perhatian khusus karena sering diserbu monyet .\", \"Monyet besar akan dikerahkan di dua stadion untuk mengusir serbuan monyet kecil .\", \"Monyet besar akan dikerahkan di dua stadion tersebut untuk mengusir serbuan monyet kecil , kata Devender Prasad , pejabat pemerintah kota Delhi kepada kantor berita AFP .\"]\n",
    "sentence = \"Monyet besar akan dikerahkan di dua stadion tersebut untuk mengusir serbuan monyet kecil , kata Devender Prasad , pejabat pemerintah kota Delhi kepada kantor berita AFP .\"\n",
    "print(\"Sentence : \", sentence)\n",
    "print(pos_tag(word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(trans_prob, emission_prob, tag_count, sentence):\n",
    "    #initialization\n",
    "    viterbi_mat = {}\n",
    "    tag_sequence = []\n",
    "    path = []\n",
    "    sentence_words = sentence.split()\n",
    "    emission = [key.split(',') for key in emission_prob.keys()] # emission probability key split\n",
    "    transmission = [key.split(',') for key in trans_prob.keys()] # transmission probability key split\n",
    "    prev_tag = [['<start>',1]]\n",
    "    best_tag = ''\n",
    "    \n",
    "    for i in range(0,len(sentence_words)):\n",
    "        temp = []\n",
    "        viterbi_mat[sentence_words[i]] = []\n",
    "        for j in emission:\n",
    "            if sentence_words[i] == j[0]:\n",
    "                value = []\n",
    "                for k in prev_tag:\n",
    "                    for l in transmission:\n",
    "                        if j[1] == l[1] :\n",
    "                            key = sentence_words[i]+','+j[1]\n",
    "                            if [k[0],j[1]] not in transmission:\n",
    "                                val = 0\n",
    "                            else :\n",
    "                                val = k[1]*trans_prob[k[0]+','+j[1]]*emission_prob[key]\n",
    "                            result = {'value' : val,'prev_tag':k[0],'tag' : j[1]}\n",
    "                            if result not in viterbi_mat[sentence_words[i]]:\n",
    "                                viterbi_mat[sentence_words[i]].append(result)\n",
    "                            if [j[1],val] not in temp:\n",
    "                                temp.append([j[1],val])\n",
    "        prev_tag = copy.copy(temp)\n",
    "    # GET BEST TAG\n",
    "    i = len(sentence_words)-1\n",
    "    max_val = 0\n",
    "    max_tag = ''\n",
    "    \n",
    "    for j in viterbi_mat[sentence_words[i]]:\n",
    "        if j['value'] > max_val:\n",
    "            max_val = j['value']\n",
    "            max_tag = j['tag']    \n",
    "    tag_sequence.insert(0, max_tag)\n",
    "    \n",
    "    while i > 0 :\n",
    "        max_val = 0\n",
    "        max_tag = ''\n",
    "        for j in viterbi_mat[sentence_words[i]]:\n",
    "            if j['value'] > max_val:\n",
    "                max_val = j['value']\n",
    "                max_tag = j['prev_tag']\n",
    "        tag_sequence.insert(0, max_tag)\n",
    "        i-=1\n",
    "    return viterbi_mat, tag_sequence\n",
    "\n",
    "import copy \n",
    "tag_count, word_tag, tag_trans = read_file_init_table('Indonesian_Manually_Tagged_Corpus.txt')\n",
    "tag_count_1, word_tag_1, tag_trans_1 = read_file_init_table('sample_postagged.txt')\n",
    "kalimat = [\"Kera untuk amankan pesta olahraga .\",\"Pemerintah kota Delhi mengerahkan monyet untuk mengusir monyet-monyet lain yang berbadan lebih kecil dari arena Pesta Olahraga Persemakmuran .\", \"Beberapa laporan menyebutkan setidaknya 10 monyet ditempatkan di luar arena lomba dan pertandingan di ibukota India .\", \"Pemkot Delhi memiliki 28 monyet dan berencana mendatangkan 10 monyet sejenis dari negara bagian Rajasthan .\", \"Jumlah monyet di ibukota India mencapai ribuan , sebagian besar , berada di kantor-kantor pemerintah dan hewan ini dianggap mengganggu ketertiban umum .\", \"Jenis monyet yang dikerahkan pemkot berbadan besar , berkekor panjang , dan memiliki wajah berwarna hitam .\", \"Monyet ini diikat dengan tali panjang dan pelatih yang mengawasi mereka akan melepas tali begitu monyet-monyet lain mendekat .\", \"Kantor berita AFP melaporkan stadion tinju dan hockey mendapatkan perhatian khusus karena sering diserbu monyet .\", \"Monyet besar akan dikerahkan di dua stadion untuk mengusir serbuan monyet kecil .\", \"Monyet besar akan dikerahkan di dua stadion tersebut untuk mengusir serbuan monyet kecil , kata Devender Prasad , pejabat pemerintah kota Delhi kepada kantor berita AFP .\"]\n",
    "\n",
    "trans_prob_1 = create_trans_prob_table(tag_trans_1, tag_count_1)\n",
    "emission_prob_1 = create_emission_prob_table(word_tag_1, tag_count_1)\n",
    "\n",
    "accuracy = []\n",
    "for i in range(len(kalimat)):\n",
    "    print(\"\\nKalimat \",i+1, \" : \", kalimat[i])\n",
    "    matrix, tag_sequence = viterbi(trans_prob,emission_prob, tag_count,kalimat[i])\n",
    "    print(\"\\nPOS tag menggunakan baseline method : \\n\",tag_sequence)\n",
    "    \n",
    "    matrix_1, tag_sequence_1 = viterbi(trans_prob_1,emission_prob_1, tag_count_1,kalimat[i])\n",
    "    print(\"POS tag berdasarkan sampel: \\n\",tag_sequence_1)\n",
    "\n",
    "    true = 0\n",
    "    for j in range(len(tag_sequence)):\n",
    "        if(tag_sequence[j] == tag_sequence_1[j]):\n",
    "            true+=1\n",
    "    accuracy.append(true/len(tag_sequence)*100)\n",
    "    print(\"Akurasi = %.2f\" % accuracy[i])\n",
    "\n",
    "print(\"\\nRata-rata akurasi = %.2f\" % (sum(accuracy)/len(kalimat)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "viterbi_postag_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
